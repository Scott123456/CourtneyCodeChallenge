# -*- coding: utf-8 -*-
"""PySparkChallenge.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GyNXGyt0yhkj-joI6UH7eSwM7SRq_Ili

# Python-PySpark Coding Challenge
Purpose: Load purchases and products, combine, and summarize for reporting.

Written by: Scott Davis 2023-03-27
"""

import pandas as pd

import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()

# show raw CSV files in filesystem - optional
!ls

# Import raw data to see structure - optional

#dfCust = spark.read.csv("CustomerPurchasesDataset.csv", header = True, quote = "\'")
#dfProd = spark.read.csv("ProductDetailsDataset.csv", header = True, quote = "\'")

#dfCust.show()
#dfCust.printSchema()

#dfProd.show()
#dfProd.printSchema()

from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType

# Assign data types
schemaCust = StructType([
    StructField('customer_id', IntegerType()),
    StructField('product_id', IntegerType()),
    StructField('purchase_amount', DoubleType())
])

schemaProd = StructType([
    StructField('product_id', IntegerType()),
    StructField('product_name', StringType()),
    StructField('unit_price', DoubleType())
])

# read raw data with schema definitions
dfCust = spark.read.csv("CustomerPurchasesDataset.csv", header = True, quote = "\'", schema = schemaCust)
dfProd = spark.read.csv("ProductDetailsDataset.csv", header = True, quote = "\'", schema = schemaProd)

# show data
dfCust.show()
dfCust.printSchema()

dfProd.show()
dfProd.printSchema()

"""## Problem 1
A PySpark DataFrame containing the joined data with the following columns:
 -	customer_id
 - 	product_id
 -	purchase_amount
 -	product_name
 -	unit_price
"""

# inner join on product_id and drop excess join column
dfJoin1 = dfCust.join(dfProd, dfCust.product_id == dfProd.product_id, how = 'inner').drop(dfProd.product_id)
dfJoin1.show()

"""## Problem 2

A PySpark DataFrame containing the total revenue generated by each product with the following columns:
 - product_id
 - product_name
 - total_revenue
"""

# use joined data, group by product, sum by purchase amount, and rename sum column
dfJoin2 = dfJoin1.groupBy('product_id', 'product_name').sum('purchase_amount').orderBy('product_id').withColumnRenamed('sum(purchase_amount)', 'total_revenue')
dfJoin2.show()

"""# Problem 3
A PySpark DataFrame containing the total revenue generated by each customer with the following columns:
 - customer_id
 - total_revenue

"""

# use joined data, group by customer, sum purchase amount, and rename sum column
dfJoin3 = dfJoin1.groupBy('customer_id').sum('purchase_amount').orderBy('customer_id').withColumnRenamed('sum(purchase_amount)', 'total_revenue')
dfJoin3.show()

"""## Problem 4
A PySpark DataFrame containing the top 5 products by revenue with the following columns:
 - product_id
 - product_name
 - total_revenue

"""

# use output from product summary above, sort descending by purchase amount, keep top 5
dfJoin4 = dfJoin2.orderBy('total_revenue', ascending = False).limit(5)
dfJoin4.show()

"""## Problem 5
A PySpark DataFrame containing the top 5 customers by revenue with the following columns:
 - customer_id
 - total_revenue

"""

# use output from customer summary above, sort descending by purchase amount, keep top 5
dfJoin5 = dfJoin3.orderBy('total_revenue', ascending = False).limit(5)
dfJoin5.show()

